# DATABASE_URL=postgresql://postgres:postgres@localhost:5432/literature_db
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/literature_db


REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/1

# ===== LLM Configuration =====
# Centralized LLM service configuration (app/llm/service.py)
# All LLM calls throughout the system use these settings

# LLM provider: "dummy" (default, safe), "openai", "nvidia", or future local models
LLM_PROVIDER=nvidia

# LLM model name (provider-specific)
# OpenAI: gpt-4o-mini, gpt-3.5-turbo, gpt-4, etc.
# NVIDIA: nvidia/llama3-chatqa-1.5-8b, etc.
LLM_MODEL=nvidia/llama3-chatqa-1.5-8b

# LLM temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.0

# LLM max tokens per response
LLM_MAX_TOKENS=800

# OpenAI API key (if using LLM_PROVIDER=openai)
# OPENAI_API_KEY=sk-your-key-here

# NVIDIA credentials (if using LLM_PROVIDER=nvidia)
NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1
NVIDIA_API_KEY=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

# ===== Prompt Files =====
DOMAIN_RESOLVER_PROMPT_FILE=domain_resolver.txt
TRIPLE_EXTRACTION_PROMPT_FILE=triple_extraction.txt
DECISION_LLM_PROMPT_FILE=decision_llm.txt
CLARIFICATION_PROMPT_FILE=clarification_question.txt
USER_CLASSIFIER_PROMPT_FILE=user_text_classifier.txt
CLARIFICATION_HIGH_PROMPT_FILE=clarification_high_ambiguity.txt
CLARIFICATION_MEDIUM_PROMPT_FILE=clarification_medium_ambiguity.txt
CLARIFICATION_LOW_PROMPT_FILE=clarification_low_ambiguity.txt

# System Safety Caps (New)
SYSTEM_MAX_PAPERS_PER_JOB=100
SYSTEM_MAX_FETCH_CYCLES=5
SYSTEM_MAX_GRAPH_SIZE=5000

