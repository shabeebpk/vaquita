# DATABASE_URL=postgresql://postgres:postgres@localhost:5432/literature_db
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/literature_db


REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/1

# ===== LLM Configuration =====
# Centralized LLM service configuration (app/llm/service.py)
# All LLM calls throughout the system use these settings

# LLM provider: "dummy" (default, safe), "openai", "nvidia", or future local models
LLM_PROVIDER=nvidia

# LLM model name (provider-specific)
# OpenAI: gpt-4o-mini, gpt-3.5-turbo, gpt-4, etc.
# NVIDIA: nvidia/llama3-chatqa-1.5-8b, etc.
LLM_MODEL=nvidia/llama3-chatqa-1.5-8b

# LLM temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.0

# LLM max tokens per response
LLM_MAX_TOKENS=800

# OpenAI API key
# OPENAI_API_KEY=sk-your-key-here

# NVIDIA credentials (if using LLM_PROVIDER=nvidia)
NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1
NVIDIA_API_KEY=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

# System Safety Caps (New)
SYSTEM_MAX_PAPERS_PER_JOB=100
SYSTEM_MAX_FETCH_CYCLES=5
SYSTEM_MAX_GRAPH_SIZE=5000

# ===== Fetch Providers =====
SEMANTIC_SCHOLAR_API_KEY=OPcW57ry2s11gzW1SmuwZ7GjBzIY07uQ6RAO5YuV
SEMANTIC_SCHOLAR_URL=https://api.semanticscholar.org/graph/v1/paper/search
# ===== Embeddings Configuration =====
# Sentence Transformers embedding model settings
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_BATCH_SIZE=32
EMBEDDING_DEVICE=cpu

# ===== Caching Configuration =====
# Redis caching settings for graph artifacts
REDIS_CACHE_TTL_SECONDS=3600
REDIS_CACHE_PREFIX=structural_graph:

# ===== Docker Infrastructure =====
# API server configuration
API_PORT=8000
# Database port mapping (host:container)
DB_PORT=5433